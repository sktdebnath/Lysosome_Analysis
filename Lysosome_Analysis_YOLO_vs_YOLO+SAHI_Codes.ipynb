{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Pred2Annotate\n",
        "This code converts model predicted contours to txt file, suitable for RoboFlow annotation."
      ],
      "metadata": {
        "id": "g_4G09VKxKER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "DpGn58sNyo92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLZ6Ij1Fwlxe"
      },
      "outputs": [],
      "source": [
        "#Use GPU enabled environment only\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from scipy.interpolate import splprep, splev\n",
        "import os\n",
        "\n",
        "# Load a pretrained YOLOv8 model\n",
        "model = YOLO(\"/content/V8_best.pt\")\n",
        "\n",
        "# Load the image and ensure proper memory handling\n",
        "image_path = \"/content/S8_00005.png\"  # Path to your uploaded image\n",
        "image      = cv2.imread(image_path)\n",
        "\n",
        "if image is None:\n",
        "    raise ValueError(f\"Image at {image_path} not found or could not be loaded.\")\n",
        "\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Get the dimensions of the image for normalization\n",
        "img_height, img_width = image_rgb.shape[:2]\n",
        "\n",
        "# Check for CUDA availability and set device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Define device\n",
        "\n",
        "# Run the model on the image with a low confidence threshold\n",
        "results = model(image_path, conf=0.10, device=device, iou=0.4,\n",
        "                retina_masks=True, imgsz=image_rgb.shape[0])\n",
        "\n",
        "# Process results and generate normalized coordinates for Roboflow\n",
        "output_filename = os.path.splitext(os.path.basename(image_path))[0] + \".txt\"\n",
        "with open(output_filename, \"w\") as file:\n",
        "    for result in results:\n",
        "        masks = result.masks  # Masks object for segmentation masks outputs\n",
        "\n",
        "        # Get the coordinates of the masks\n",
        "        xy = masks.xy  # List of arrays with (x, y) coordinates of mask vertices\n",
        "\n",
        "        for i, polygon in enumerate(xy):\n",
        "            polygon = polygon.astype(np.float32)  # Ensure coordinates are float for normalization\n",
        "\n",
        "            # Apply spline smoothing to the polygon coordinates\n",
        "            tck, u = splprep(polygon.T, s=100.0)  # s is the smoothing factor\n",
        "            new_points = splev(np.linspace(0, 1, 20), tck)\n",
        "            new_points = np.stack(new_points, axis=-1)  # Shape (n_points, 2)\n",
        "\n",
        "            # Normalize the coordinates\n",
        "            normalized_coords = []\n",
        "            for point in new_points:\n",
        "                norm_x = point[0] / img_width\n",
        "                norm_y = point[1] / img_height\n",
        "                normalized_coords.append(f\"{norm_x} {norm_y}\")\n",
        "\n",
        "            # Write the normalized coordinates to the text file\n",
        "            file.write(\"0 \" + \" \".join(normalized_coords) + \"\\n\")\n",
        "\n",
        "# Explicitly release memory\n",
        "del image\n",
        "del image_rgb\n",
        "del results\n",
        "torch.cuda.empty_cache()  # Clears cache on the GPU, if being used\n",
        "\n",
        "print(f\"Annotations saved to {output_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training An YOLOv8 Segmentation Model"
      ],
      "metadata": {
        "id": "7pkhokOVzfMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "qoeUi6Oczk9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: You can use one of our data from github or can upload your dataset here.\n",
        "!git clone https://github.com/sktdebnath/Lysosome_Analysis.git\n",
        "!unzip -q '/content/Lysosome_Analysis/V8 Training Dataset.zip'"
      ],
      "metadata": {
        "id": "lpuBxq_mzr4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import torch\n",
        "\n",
        "# Check for CUDA availability and set device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Define device\n",
        "\n",
        "# Initialize the model\n",
        "model = YOLO('yolov8s-seg.pt')\n",
        "\n",
        "# Define the training settings\n",
        "train_settings = {\n",
        "    'data'    : '/content/V8_Training_Dataset/data.yaml', # Your YAML Path.\n",
        "    # Open the yaml file and change the location of train, val and test files properly.\n",
        "    # For example:\n",
        "    #     train: /content/V8 Training Dataset/train\n",
        "    #     val: train: /content/V8 Training Dataset/valid\n",
        "    'epochs'  : 200,\n",
        "    'cos_lr'  : True,\n",
        "    'patience': 50,\n",
        "    'seed'    : 42,\n",
        "    'batch'   :-1,\n",
        "    'imgsz'   : 640,\n",
        "    'device'  : device,\n",
        "    'plots'   : True\n",
        "}\n",
        "\n",
        "# Train the model with the specified settings\n",
        "model.train(**train_settings)\n",
        "\n",
        "# Empty cache for an efficient memory management.\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "g7icB8dY0ECK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#YOLO Inference\n",
        "This code batchprocess multiple files at a time. All contours are preserved in the output directory."
      ],
      "metadata": {
        "id": "87zKiISD2tgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "from sahi import AutoDetectionModel\n",
        "from sahi.predict import get_sliced_prediction\n",
        "from sahi.predict import get_prediction\n",
        "from scipy.interpolate import splprep, splev  # For B-spline interpolation\n",
        "import time  # For measuring inference time\n",
        "\n",
        "def process_image(image_path, output_dir, model, detection_model, device):\n",
        "    # Load the image\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        print(f\"Image not found or could not be loaded: {image_path}\")\n",
        "        return\n",
        "\n",
        "    # Measure inference time\n",
        "    start_time = time.time()  # Start timer\n",
        "\n",
        "    # Perform YOLO inference   ########################################Changed Here\n",
        "    sliced_result = get_prediction(\n",
        "        image,  # Input image\n",
        "        detection_model\n",
        "    )\n",
        "\n",
        "    # End timer\n",
        "    inference_time = (time.time() - start_time) * 1000  # Convert to milliseconds\n",
        "\n",
        "    # Measure GPU memory usage\n",
        "    gpu_memory_allocated = torch.cuda.memory_allocated() / 1e6  # Convert to MB\n",
        "    gpu_memory_reserved = torch.cuda.memory_reserved() / 1e6  # Convert to MB\n",
        "    gpu_memory_max = torch.cuda.max_memory_allocated() / 1e6  # Convert to MB\n",
        "\n",
        "    # Create a copy of the original image for drawing\n",
        "    output_image = image.copy()\n",
        "\n",
        "    # Process each prediction\n",
        "    for prediction in sliced_result.object_prediction_list:\n",
        "        try:\n",
        "            # Extract mask\n",
        "            mask = prediction.mask\n",
        "            mask_array = mask.bool_mask  # Access the binary mask array directly\n",
        "\n",
        "            # Validate the mask\n",
        "            if mask_array is None or mask_array.sum() == 0:\n",
        "                print(f\"Skipping invalid or empty mask in {image_path}.\")\n",
        "                continue\n",
        "\n",
        "            # Find contours from the binary mask\n",
        "            contours, _ = cv2.findContours(mask_array.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "            # Process each contour\n",
        "            for contour in contours:\n",
        "                contour = contour[:, 0, :]  # Simplify contour array\n",
        "\n",
        "                # Calculate the centroid of the contour\n",
        "                M = cv2.moments(contour)\n",
        "                if M[\"m00\"] != 0:\n",
        "                    cX = int(M[\"m10\"] / M[\"m00\"])\n",
        "                    cY = int(M[\"m01\"] / M[\"m00\"])\n",
        "                else:\n",
        "                    cX, cY = 0, 0\n",
        "\n",
        "                # Smoothen the contour using B-spline interpolation\n",
        "                if len(contour) > 3:  # Ensure there are enough points for interpolation\n",
        "                    tck, u = splprep(contour.T, s=100.0)  # s is the smoothing factor\n",
        "                    new_points = splev(np.linspace(0, 1, 50), tck)  # Generate 50 new points\n",
        "                    new_points = np.stack(new_points, axis=-1).astype(np.int32)\n",
        "\n",
        "                    # Scale the contour points to 80% of the original size relative to the centroid\n",
        "                    scaled_points = []\n",
        "                    for point in new_points:\n",
        "                        # Translate the point so that the centroid is at the origin\n",
        "                        translated_x = point[0] - cX\n",
        "                        translated_y = point[1] - cY\n",
        "                        # Scale the translated point by 0.8 (80%)\n",
        "                        scaled_x = translated_x * 1.\n",
        "                        scaled_y = translated_y * 1.\n",
        "                        # Translate the point back to the original coordinate system\n",
        "                        scaled_points.append([scaled_x + cX, scaled_y + cY])\n",
        "                    scaled_points = np.array(scaled_points, dtype=np.int32)\n",
        "\n",
        "                    # Shift the scaled contour points 2% towards the bottom-right direction\n",
        "                    shift_factor = 0.001  # .1% shift\n",
        "                    shift_x = int((image.shape[1] - cX) * shift_factor)  # Shift in x-direction\n",
        "                    shift_y = int((image.shape[0] - cY) * shift_factor)  # Shift in y-direction\n",
        "                    shifted_points = scaled_points + [shift_x, shift_y]\n",
        "\n",
        "                    # Draw the scaled, shifted, and smoothened contour\n",
        "                    cv2.polylines(output_image, [shifted_points], isClosed=True, color=(0, 0, 255), thickness=2)  # Red contour\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping prediction in {image_path} due to error: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Add text to display the total number of instances\n",
        "    text = f\"Total Instances: {len(sliced_result.object_prediction_list)}\"\n",
        "    cv2.putText(output_image, text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 3)\n",
        "\n",
        "    # Convert image from BGR to RGB for display\n",
        "    output_image_rgb = cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Save the annotated image to the output directory\n",
        "    output_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "    cv2.imwrite(output_path, output_image)\n",
        "\n",
        "    # Print GPU RAM usage, inference time, and contour count\n",
        "    print(f\"File: {os.path.basename(image_path)} | GPU RAM Allocated: {gpu_memory_allocated:.2f} MB | GPU RAM Reserved: {gpu_memory_reserved:.2f} MB | GPU RAM Max Allocated: {gpu_memory_max:.2f} MB | Inference Time: {inference_time:.2f} ms | Contours Detected: {len(sliced_result.object_prediction_list)}\")\n",
        "\n",
        "def run_inference_on_directory(input_dir, output_dir):\n",
        "    # Check if GPU is available\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load a pre-trained YOLO model (e.g., YOLOv8)\n",
        "    model_path = '/content/V10_best.pt'  # Replace with your YOLO model path\n",
        "    model      = YOLO(model_path)\n",
        "\n",
        "    # Override default parameters\n",
        "    model.overrides['retina_masks'] = True  # Set retina mask\n",
        "    model.overrides['max_det']      = 600   # Set your desired maximum number of detections\n",
        "    model.overrides['conf']         = 0.4   # Confidence threshold\n",
        "    model.overrides['iou']          = 0.4   # IoU threshold for NMS\n",
        "    model.overrides['imgsz']        = 2240  # Input image size\n",
        "    model.overrides['agnostic_nms'] = True  # Class-agnostic NMS\n",
        "    model.overrides['mask_ratio']   = 2     # Downsampling ratio for masks\n",
        "    model.to(device)                        # Move model to GPU\n",
        "\n",
        "    # Initialize SAHI's AutoDetectionModel with YOLO\n",
        "    detection_model = AutoDetectionModel.from_pretrained(\n",
        "        model_type           = 'ultralytics',  # Specify model type\n",
        "        model                = model,  # Pass the modified YOLO model\n",
        "        confidence_threshold = 0.4,  # Adjust confidence threshold as needed\n",
        "        device               = device,  # Use GPU\n",
        "    )\n",
        "\n",
        "    # Create the output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Process each image in the input directory\n",
        "    for filename in os.listdir(input_dir):\n",
        "        if filename.lower().endswith(('.png')):\n",
        "            image_path = os.path.join(input_dir, filename)\n",
        "            process_image(image_path, output_dir, model, detection_model, device)\n",
        "\n",
        "            # Clear GPU memory after processing each image\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "# Define input and output directories\n",
        "input_dir  = '/content'  # Replace with your input directory\n",
        "output_dir = '/content/Output_YOLO'  # Replace with your output directory\n",
        "\n",
        "# Run inference on all images in the input directory\n",
        "run_inference_on_directory(input_dir, output_dir)"
      ],
      "metadata": {
        "id": "tlwmPSvc1ekI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#YOLO+SAHI Inference"
      ],
      "metadata": {
        "id": "7qUHe6803ScW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "from sahi import AutoDetectionModel\n",
        "from sahi.predict import get_sliced_prediction\n",
        "from scipy.interpolate import splprep, splev  # For B-spline interpolation\n",
        "import time  # For measuring inference time\n",
        "\n",
        "def process_image(image_path, output_dir, model, detection_model, device):\n",
        "    # Load the image\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        print(f\"Image not found or could not be loaded: {image_path}\")\n",
        "        return\n",
        "\n",
        "    # Measure inference time\n",
        "    start_time = time.time()  # Start timer\n",
        "\n",
        "    # Perform sliced inference using SAHI\n",
        "    sliced_result = get_sliced_prediction(\n",
        "        image,  # Input image\n",
        "        detection_model,  # SAHI detection model\n",
        "        slice_height=640,  # Height of each slice\n",
        "        slice_width=640,  # Width of each slice\n",
        "        overlap_height_ratio=0.2,  # Overlap ratio between slices\n",
        "        overlap_width_ratio=0.2,\n",
        "        postprocess_type=\"NMS\"\n",
        "    )\n",
        "\n",
        "    # End timer\n",
        "    inference_time = (time.time() - start_time) * 1000  # Convert to milliseconds\n",
        "\n",
        "    # Measure GPU memory usage\n",
        "    gpu_memory_allocated = torch.cuda.memory_allocated() / 1e6  # Convert to MB\n",
        "    gpu_memory_reserved = torch.cuda.memory_reserved() / 1e6  # Convert to MB\n",
        "    gpu_memory_max = torch.cuda.max_memory_allocated() / 1e6  # Convert to MB\n",
        "\n",
        "    # Create a copy of the original image for drawing\n",
        "    output_image = image.copy()\n",
        "\n",
        "    # Process each prediction\n",
        "    for prediction in sliced_result.object_prediction_list:\n",
        "        try:\n",
        "            # Extract mask\n",
        "            mask = prediction.mask\n",
        "            mask_array = mask.bool_mask  # Access the binary mask array directly\n",
        "\n",
        "            # Validate the mask\n",
        "            if mask_array is None or mask_array.sum() == 0:\n",
        "                print(f\"Skipping invalid or empty mask in {image_path}.\")\n",
        "                continue\n",
        "\n",
        "            # Find contours from the binary mask\n",
        "            contours, _ = cv2.findContours(mask_array.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "            # Process each contour\n",
        "            for contour in contours:\n",
        "                contour = contour[:, 0, :]  # Simplify contour array\n",
        "\n",
        "                # Calculate the centroid of the contour\n",
        "                M = cv2.moments(contour)\n",
        "                if M[\"m00\"] != 0:\n",
        "                    cX = int(M[\"m10\"] / M[\"m00\"])\n",
        "                    cY = int(M[\"m01\"] / M[\"m00\"])\n",
        "                else:\n",
        "                    cX, cY = 0, 0\n",
        "\n",
        "                # Smoothen the contour using B-spline interpolation\n",
        "                if len(contour) > 3:  # Ensure there are enough points for interpolation\n",
        "                    tck, u = splprep(contour.T, s=100.0)  # s is the smoothing factor\n",
        "                    new_points = splev(np.linspace(0, 1, 50), tck)  # Generate 50 new points\n",
        "                    new_points = np.stack(new_points, axis=-1).astype(np.int32)\n",
        "\n",
        "                    # Scale the contour points to 90% of the original size relative to the centroid\n",
        "                    scaled_points = []\n",
        "                    for point in new_points:\n",
        "                        # Translate the point so that the centroid is at the origin\n",
        "                        translated_x = point[0] - cX\n",
        "                        translated_y = point[1] - cY\n",
        "                        # Scale the translated point by 0.9 (90%)\n",
        "                        scaled_x = translated_x * 1.\n",
        "                        scaled_y = translated_y * 1.\n",
        "                        # Translate the point back to the original coordinate system\n",
        "                        scaled_points.append([scaled_x + cX, scaled_y + cY])\n",
        "                    scaled_points = np.array(scaled_points, dtype=np.int32)\n",
        "\n",
        "                    # Shift the scaled contour points 2% towards the bottom-right direction\n",
        "                    shift_factor = 0.001  # .1% shift\n",
        "                    shift_x = int((image.shape[1] - cX) * shift_factor)  # Shift in x-direction\n",
        "                    shift_y = int((image.shape[0] - cY) * shift_factor)  # Shift in y-direction\n",
        "                    shifted_points = scaled_points + [shift_x, shift_y]\n",
        "\n",
        "                    # Draw the scaled, shifted, and smoothened contour\n",
        "                    cv2.polylines(output_image, [shifted_points], isClosed=True, color=(0, 0, 255), thickness=2)  # Red contour\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping prediction in {image_path} due to error: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Add text to display the total number of instances\n",
        "    text = f\"Total Instances: {len(sliced_result.object_prediction_list)}\"\n",
        "    cv2.putText(output_image, text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 3)\n",
        "\n",
        "    # Convert image from BGR to RGB for display\n",
        "    output_image_rgb = cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Save the annotated image to the output directory\n",
        "    output_path = os.path.join(output_dir, os.path.basename(image_path))\n",
        "    cv2.imwrite(output_path, output_image)\n",
        "\n",
        "    # Print GPU RAM usage, inference time, and contour count\n",
        "    print(f\"File: {os.path.basename(image_path)} | GPU RAM Allocated: {gpu_memory_allocated:.2f} MB | GPU RAM Reserved: {gpu_memory_reserved:.2f} MB | GPU RAM Max Allocated: {gpu_memory_max:.2f} MB | Inference Time: {inference_time:.2f} ms | Contours Detected: {len(sliced_result.object_prediction_list)}\")\n",
        "\n",
        "def run_inference_on_directory(input_dir, output_dir):\n",
        "    # Check if GPU is available\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load a pre-trained YOLO model (e.g., YOLOv8)\n",
        "    model_path = '/content/V10_best.pt'  # Replace with your YOLO model path\n",
        "    model      = YOLO(model_path)\n",
        "\n",
        "    # Override default parameters\n",
        "    model.overrides['retina_masks'] = True  # Set retina mask\n",
        "    model.overrides['conf']         = 0.4   # Confidence threshold\n",
        "    model.overrides['iou']          = 0.4   # IoU threshold for NMS\n",
        "    model.overrides['imgsz']        = 640   # Input image size\n",
        "    model.overrides['agnostic_nms'] = True  # Class-agnostic NMS\n",
        "    model.overrides['mask_ratio']   = 2     # Downsampling ratio for masks\n",
        "    model.to(device)                        # Move model to GPU\n",
        "\n",
        "    # Initialize SAHI's AutoDetectionModel with YOLO\n",
        "    detection_model = AutoDetectionModel.from_pretrained(\n",
        "        model_type           = 'ultralytics',  # Specify model type\n",
        "        model                = model,  # Pass the modified YOLO model\n",
        "        confidence_threshold = 0.4,  # Adjust confidence threshold as needed\n",
        "        device               = device,  # Use GPU\n",
        "    )\n",
        "\n",
        "    # Create the output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Process each image in the input directory\n",
        "    for filename in os.listdir(input_dir):\n",
        "        if filename.lower().endswith(('.png')):\n",
        "            image_path = os.path.join(input_dir, filename)\n",
        "            process_image(image_path, output_dir, model, detection_model, device)\n",
        "\n",
        "            # Clear GPU memory after processing each image\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "# Define input and output directories\n",
        "input_dir  = '/content'  # Replace with your input directory\n",
        "output_dir = '/content/Output_SAHI'  # Replace with your output directory\n",
        "\n",
        "# Run inference on all images in the input directory\n",
        "run_inference_on_directory(input_dir, output_dir)"
      ],
      "metadata": {
        "id": "Widsroku3Ved"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#YOLO vs YOLO+SAHI Detection Comparison:"
      ],
      "metadata": {
        "id": "Zz6p_T3639bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "from sahi import AutoDetectionModel\n",
        "from sahi.predict import get_sliced_prediction\n",
        "from sahi.predict import get_prediction\n",
        "\n",
        "# Check if GPU is available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load a pre-trained YOLO model (e.g., YOLOv8)\n",
        "model_path = '/content/best.pt'  # Replace with your YOLO model path\n",
        "model      = YOLO(model_path)\n",
        "\n",
        "# Override default parameters\n",
        "model.overrides['max_det']      = 500  # Set your desired maximum number of detections\n",
        "model.overrides['iou']          = 0.4  # Set your desired IOU\n",
        "model.overrides['retina_masks'] = True # Set retina mask\n",
        "model.to(device)                  # Move model to GPU\n",
        "\n",
        "# Load an image (replace with your image path)\n",
        "image_path = '/content/5_frame_1.png'  # Upload your image to Colab or provide a path\n",
        "image      = cv2.imread(image_path)\n",
        "\n",
        "# Ensure the image is loaded\n",
        "if image is None:\n",
        "    raise FileNotFoundError(f\"Image not found at {image_path}\")\n",
        "\n",
        "# Initialize SAHI's AutoDetectionModel with YOLO\n",
        "detection_model = AutoDetectionModel.from_pretrained(\n",
        "    model_type           = 'ultralytics',  # Specify model type\n",
        "    model                = model,          # Pass the modified YOLO model\n",
        "    confidence_threshold = 0.4,            # Adjust confidence threshold as needed\n",
        "    device               = device          # Use GPU\n",
        ")\n",
        "\n",
        "# Perform standard YOLO prediction\n",
        "YOLO_result = get_prediction(image_path, detection_model)\n",
        "\n",
        "# Perform sliced inference using SAHI\n",
        "sliced_result = get_sliced_prediction(\n",
        "    image,                       # Input image\n",
        "    detection_model,             # SAHI detection model\n",
        "    slice_height         = 640,  # Height of each slice\n",
        "    slice_width          = 640,  # Width of each slice\n",
        "    overlap_height_ratio = 0.2,  # Overlap ratio between slices\n",
        "    overlap_width_ratio  = 0.2\n",
        ")\n",
        "\n",
        "for prediction in YOLO_result.object_prediction_list:\n",
        "    x1, y1, x2, y2 = prediction.bbox.minx, prediction.bbox.miny, prediction.bbox.maxx, prediction.bbox.maxy\n",
        "    centroid_x     = int((x1 + x2) / 2)  # Calculate centroid x-coordinate\n",
        "    centroid_y     = int((y1 + y2) / 2)  # Calculate centroid y-coordinate\n",
        "    # Draw centroid on the image\n",
        "    cv2.circle(image, (centroid_x, centroid_y), 7, (0, 255, 0), -1)  # Green dot for YOLO centroid\n",
        "\n",
        "for prediction in sliced_result.object_prediction_list:\n",
        "    x1, y1, x2, y2 = prediction.bbox.minx, prediction.bbox.miny, prediction.bbox.maxx, prediction.bbox.maxy\n",
        "    centroid_x     = int((x1 + x2) / 2)  # Calculate centroid x-coordinate\n",
        "    centroid_y     = int((y1 + y2) / 2)  # Calculate centroid y-coordinate\n",
        "    # Draw centroid on the image\n",
        "    cv2.circle(image, (centroid_x, centroid_y), 7, (0, 0, 255), -1)  # Red dot for SAHI centroid\n",
        "\n",
        "# Add text to display the total number of centroids with YOLO and SAHI\n",
        "YOLO_text = f\"Total Centroids YOLO      : {len(YOLO_result.object_prediction_list)}\"\n",
        "SAHI_text = f\"Total Centroids YOLO+SAHI: {len(sliced_result.object_prediction_list)}\"\n",
        "\n",
        "cv2.putText(image, YOLO_text, (10, 60),  cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 3)\n",
        "cv2.putText(image, SAHI_text, (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 3)\n",
        "\n",
        "# Convert image from BGR to RGB for display\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display the image with centroids\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(image_rgb)\n",
        "plt.axis('off')\n",
        "plt.title(\"Detected Objects with Centroids (YOLO and SAHI)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "890pYcXc4CGN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}